%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2022}{Austin, TX}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{What's in a Domain? Anaylsis of URL Features}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{John Hawkins}
\email{john.hawkins@Getting-Data-Science-Done.com}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Getting-Data-Science-Done.com}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
  \postcode{2000}
}

\renewcommand{\shortauthors}{Hawkins}

\begin{abstract}
Many data science problems require processing log data derived from web pages, apis or other
internet traffic sources. URLs are one of the few ubiquitous data fields that describe
internet activity, hence they require effective processing for a wide variety of machine 
learning applications. While URLs are structurally rich, they are not subject
to universal principles of construction making fetaure engineering for internet data an ongoing challenge.

In this research we outline the key categories of information structure in domains and URLs.
We share an open source implementation of these ideas and demonstrate their utility across a range of 
internet traffic machine learning problems. Python package available at https://pypi.org/project/url2features
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003260.10003261</concept_id>
<concept_desc>Information systems~Web searching and information discovery</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003260.10003277.10003280</concept_id>
<concept_desc>Information systems~Web log analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Web searching and information discovery}
\ccsdesc[500]{Information systems~Web log analysis}
\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{URL Features, Document Classification, Machine Learning, Semantic Web}

\maketitle

\section{Introduction}

The Uniform Resource Locator (URL) is a ubiquitous element the digital world. 
We use them to advertise and locate businesses, to retrieve news and media and 
to interact with other people across a broad of social applications.
Processing URLs is key component of many tasks that involve
analysing internet data. In many applications the URL is the central piece of 
information available because the demands of the task require immediate analysis.
In applications like malicious wesite detection the URL needs to be processed with
in a rapid and efficient manner to provide utility\cite{Ma2009b}.
Common use cases for URL centered analysis include security analysis of potenital phishing 
attacks\cite{Garera2007,Basnet2012,Basnet2014,Mamun2016,Verma2017,Vazhayil2018,Tupsamudre2019,Li2020}, 
and identification of pages that host malware or viruses \cite{Canali2011,Mamun2016}.
There are also applications to online advertising,
including anticipation of conversions \cite{Qiu2020}, contextual analysis of
the content\cite{Kan2004,Shih2004,Baykan2009,Meshkizadeh2010,Hernandez2012,Arya2016}, relevance \cite{Kan2005} 
or language\cite{Baykan2013} of webpages. 
Content categorisation has also been applied to spam web pages using URLs alone for the sake
of search results filtering\cite{Chung2010}.

The URL is made up of multiple elements, but at its core is the domain. The domain
contains internal information about both its intended purpose, legitimacy and likely
origin. It is also a source of additional information through requests to Domain Name
Servers (DNS) to understand both its history and place within the network topology.

Common feature engineering strategies exploit these potential sources of information
in ways that are appropriate for a given task. However, while there are wide variety
of approaches that inconsistently applied, there has not yet been a study that evaluates
the overall effectiveness of different strategies across a range of applications.

In this work we develop a feature engineering library that maintains an ontology of URL features 
that relate to underlying structure of the URL, with a method for visualising the importance of features
across the URL structure. These techniques can prove useful for analysing features for problems that
consist of moving targets and require broad sets of potential features and efficient feature selection \cite{Basnet2012}. 
We then present an open source package for generating features within these
ontologocal classes and apply them to a variety of tasks. We present our results as a
cross-task evaluation of URL features.  
 
A large amount of recent work on machine learning based on URLs has focused on deep learning approaches that
learn an embedding representation of the content of the URL. Once of the explicit reasons 
for this approach is the potential for sensitivity to the sequential structure of the URL.
\cite{Le2018}

Deep learning approaches include the use of convolutional layers to learn word and character 
embedings \cite{Le2018},


Classify Web pages based on URLs from the 4 Universities WebKB dataset. Comparing features derived from segmentation
of the URL to features derived from either link anchor text or the web page document itself \cite{Kan2004}.

Common approaches to feature engineering on URLs include string patterns and regular expressions to
identify key sequences and lexical properties \cite{Kan2004,Garera2007,Mamun2016,Tupsamudre2019}, 
creation of lookup tables or likelihood scores based on key sequences
\cite{Meshkizadeh2010}, n-gram or bag of words models \cite{Baykan2009,Verma2017}, 
the generation of task specific embedding vectors\cite{Le2018,Qiu2020} and the
usage of domain name servers or registrars for ancillary information about the domain registration 
and server configuration\cite{Canali2011,Li2020}. In many modern approaches to malicious URL detection
multiple feature engineering approaches are typically combined\cite{Sayamber2014,Li2020}. Increasingly,
sophisticated methods are used to combine, select or learn from combinations of features to adapt to
changing requirements, particularly in internet security applications\cite{Sountharrajan2020,Li2020}.

Some authors have 
emphasized that URLs are sequences of characters and require feature extraction methods that respect 
this sequential nature\cite{Le2018,Vazhayil2018}. They have naturally turned to developing neural network approaches
based on convolutional or recurrent layers for learning these sequential structures.
However, the ordering of key features within URLs is predominantly fixed as shown in Figure \ref{fig:url_structure}.
The core elements of URL structure occur at fixed positions within the sequence, in contrast to human language
sequences which are built from flexible grammars that allow variable positioning of most key elements. The
inherent structure of URLs has been shown to be a source of information that can be exploited in algorithm design \cite{Shih2004}.
Furthermore,
there are psychological and social elements to URL construction (such as the use of common brand names in subdomains
for phishing attack URLS \cite{Tupsamudre2019}), which emphasize the utility of feature engineering techniques that
respect the inherent structure of URLs.

\begin{figure*}
\centering
\includegraphics[scale=0.3]{images/URL_parts.png}
\caption{Structural components of a URL requiring specific feature engineering treatment}
\label{fig:url_structure}
\end{figure*}



\section{Methodology}

We design a feature extraction library for URLs that is sensitive to the various sections of URL
structure outlined above. We then apply this library to multiple machine learning tasks using only URLs
as the input variable. We experiment with multiple standard machine learning techniques and evaluate
the impact of the URL features using the SHAP package for feature importance.

\subsection{Features}

We generate URL features in groups that focus on distinct regions across the URL. We create a group
of global features that contain general string properties of the entire URL, then create regions
specific features as outlined in Table \ref{tab:features}.

\begin{table*}
\caption{URL Features}
\label{tab:features}
\begin{tabular}{|l|l|l|l|}
\toprule
Group         &Feature              &Type        &Definition  \\
\midrule
Protocol      &protocol\_name         &Category    &The exact internet protocol name    \\
              &protocol\_type         &Category    &Internet protocol categorised by its purpose    \\
              &protocol\_exists       &Boolean     &Flag indicating presence or absence of internet protocol   \\
\midrule
Host          &host\_is\_ip           &Boolean     &Flag indicating if the host is an IP address    \\
              &host\_has\_port        &Boolean     &Flag indicating if a port number is explicitly specified    \\
              &domain\_sections       &Numeric     &Count of period separated domain sections    \\
              &domain\_reg\_year      &Numeric     &Year in which the domain was initially registered    \\
              &subdomain\_type        &Category    &Categorisation of the purpose of the subdomain \\
              &subdomain\_freq        &Numeric     &Frequency of the subdomain in internet traffics    \\
              &tld\_name              &Category    &Top Level Domain Name    \\
              &tld\_type              &Category    &Top Level Domain extension categorised by purpose    \\
              &tld\_freq              &Numeric     &Top Level Domain extension frequency in internet traffic   \\
\midrule
Path          &path\_depth            &Numeric     &The depth of the directory path between host and file   \\
              &path\_1st\_wd          &Category    &The first distinct word in the path longer than 2 chars  \\
              &path\_1st\_wd\_prefix  &Category    &The first 3 chars of the first distinct word in the path > 2 chars   \\
              &path\_wd\_count        &Numeric     &Count of distinct words in the path between host and file   \\
              &path\_wd\_len          &Numeric     &The mean length of the individual words in the path  \\
              &path\_has\_date        &Boolean     &Flag indicating if a date is detected in the path  \\
              &path\_is\_home         &Boolean     &Flag indicating if the path starts with a home directory    \\
\midrule 
File          &file\_len              &Numeric     &The character length of the target file    \\
              &file\_1st\_wd          &Category    &The first distinct word in the file name  \\
              &file\_1st\_wd\_prefix  &Category    &The first 3 chars of the first distinct word in the filename  \\
              &file\_wd\_count        &Numeric     &The count of words longer than 2 chars in the file name   \\
              &file\_wd\_len          &Numeric     &The mean length of the individual words in the file name  \\
              &file\_ext              &Category    &The exact file extension e.g. "exe" or "php"    \\
              &file\_type             &Category    &File extension categorised by function \& purpose (e.g. Static or dynamic)   \\
              &file\_ext\_exists      &Boolean     &Flag indicating presence of a file extension for the URL target  \\
\midrule
Params        &params\_len            &Numeric     &The character length of the parameter string    \\
              &params\_count          &Numeric     &The count of the key value pairs in the param string    \\
              &params\_match          &Boolean     &Flag indicating if the count of keys and values in the param string match    \\
              &params\_has\_url       &Boolean     &Flag indicating presence of a raw URL within the parameter string   \\
              &params\_enc\_url       &Boolean     &Flag indicating presence of an encoded URL within the parameter string   \\
              &params\_enc\_char      &Boolean     &Flag indicating presence of an encoded character within the parameter string   \\
              &params\_frag\_len      &Numeric     &Length of a hash delineated fragement at the end of the parameter string   \\
              &keys\_len              &Numeric     &The mean length of the individual keys in the params  \\
              &keys\_numeric          &Numeric     &The proportion of numeric characters in the param keys \\
              &values\_len            &Numeric     &The mean length of the individual values in the params  \\
              &values\_numeric        &Numeric     &The proportion of numeric characters in the param values \\
\bottomrule
\end{tabular}
\end{table*}


These URL feature functions are available in our open source implmentation and python package \emph{url2features}. 
The package is designed such that it can process large files in chunks and will only add the specific sub-groups of
features a user requests.


\subsection{Data}

The data used in this study was colated from a range of publications that 
present internet classification problems containing URLs as the primary feature.
The ISCX Malcious URL classification dataset contains both malware and phishing URLS 
that need to be discriminated against benign URLS\cite{Mamun2016}.
The world wide web knowledge base (WebKb) 4 Universities data set contains a wide
range of university URLs categorised into multiple topic categories\cite{Craven1998}.
The Syskill \& Webert webpage ratings dataset containing webepages across 4 categories
with a human generated categorisation for determining personal preferences in webpage
content\cite{Pazzani1996}. These 4 URL classification problems are summarised in Table \ref{tab:data}

\begin{table}
\caption{Datasets}
\label{tab:data}
\begin{tabular}{|l|r|r|r|}
\toprule
Dataset              &Records        &Classes  &Majority    \\
\midrule
Malware              &46,944          &2       &75\%        \\
Phishing             &45,343          &2       &78\%        \\
WebKb 4Uni           &8,284           &7       &45\%        \\
Syskill \& Webert    &330             &3       &68\%        \\
DMOZ                 &1,562,978       &15      &16\%        \\
\bottomrule
\end{tabular}
\end{table}

We generate all url features for each of these datasets and then apply a range of machine learning
techniques to build classifiers that can be analysed for key features for each task.

\subsection{Feature Importance}

We use the SHAP\cite{Lundberg2017} package to calculate feature importance using a small holdout test set 
of 80-100 smaples per experiment. We aggregate the shapley values for these sample points to calculate the mean
absolute contribution of each feature, as is typically done in Shaplet fetaure importance plots.

For our URL feature analysis, we then group these individual feature importance values by the specific URL segment
that the feature was derive from, using Figure \ref{fig:url} as a guide to these segments. 
We sum the feature importance for all features within a URL segment to allow us to plot the
segment contributions to model performance. We provide a thin grey coloured line covering the entire segment
which captures the importance of global features (like URL length). If the script is applied to a dataset with
more than just URL features then all non-URL fetaures would be grouped into this global group. All segment specific
URL features are plotted below their segment in a canonical example URL.

These plots allow for structural understanding of how URL data
controbutes to predictive performance of a model and permits the comparison of feature importance 
across the five problems in this study. The script for generating these plots is provides as part of the
`url2features` open source package.

\section{Results}

A set of standard machine learning pipelines are applied to each of the four problems. Each approach is applied
using default parameters, without any fine tuning. We use feature preprocessing modules appropriate for each class
of model. All details are available in the source repository for the experiments. The performance of the models
is tabulated in Table \ref{tab:results}, where we show Balanced Accuracy of each model on the holdout data, as this
metric is appropriate and comparable across all four problems. 

\begin{table}
\caption{Machine Learning Results}
\label{tab:results}
\begin{tabular}{|l|r|r|r|r|}
\toprule
Dataset              &NB     &LR    &XT    &LGBM     \\
\midrule
Malware              &0.65   &0.93  &0.99  &1.00     \\
Phishing             &0.89   &0.98  &0.99  &1.00     \\
WebKb 4Uni           &0.30   &0.36  &0.58  &0.49     \\
Syskill \& Webert    &0.39   &0.35  &0.37  &0.33     \\
DMOZ                 &0.12   &0.17  &0.29  &0.25     \\
\bottomrule
\end{tabular}
\end{table}

We take the best performing model for each problem and generate the SHAP values for the records in the test data.
These records are then used to generate the URL Structure Feature Importance Plot shown in Figure \ref{fig:importance}. 

\begin{figure*}
\centering
\includegraphics[scale=0.8]{images/URL_importance.png}
\caption{Structural Depiction of URL Feature Importance}
\label{fig:importance}
\end{figure*}

We see that for all tasks in these experiments the `path` segment provides the most consistent contribution. The subdomain
and global fetaures appear to be strong for all of the malicious URL detection problems.

\section{Conclusion}

We have described the structural elements of a URL from which context specific features can be extracted.
These feature ideas tend to be used indepednetly within different domains. In this paper we have have 
collated them into an open source URL feature generation package and then compare their utility across a range
of indepenent tasks. We have observed that the various structural regions play different roles across 
these tasks, exhibiting varying levels of importance.  



\section{Acknowledgments}

\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
